**Title**: FILIP: Fine-grained Interactive Language-Image Pre-Training
**Venue**: ICLR 2022

**Reviewer**: HyeongJun Do
**Last Updated**: 06.05.2024
**Refence**
> Paper Link: https://arxiv.org/abs/2111.07783

**A Brief Overview**
> - 기존 Unsupervised large-scale vision-language pre-training 방식은 downstream tasks에서 뛰어난 성능을 발휘함.
> 	- 이는 각 modality 간의 global feature의 관계를 학습을 하지만, 더 세밀한(finer-grained) 부분을 학습하지 못함.
> - 하지만, viasual and textual tokens를 사용한 cross/self-attention 방식은 training과 inference 두 과정 모두 효율적이지 못한 결과를 보임.
> - 이에 따라, cross-modal late interaction 방식을 제안함
> 	- token-wise maximun similarity to guide contrastive objective.
> 	- contrastive objective만 수정함.

<details> 
<summary>1. Introduction</summary>
<div markdown="1"> 
안녕 
</div> 
</details>

<details> 
<summary>2. Related Work</summary>
<div markdown="1"> 
안녕 
</div> 
</details>


<details> 
<summary>3. Method</summary>
<div markdown="1"> 
안녕 
</div> 
</details>


<details> 
<summary>4. Experiments</summary>
<div markdown="1"> 
안녕 
</div> 
</details>

<details> 
<summary>5. Conclusion and Future Work</summary>
<div markdown="1"> 
안녕 
</div> 
</details>
