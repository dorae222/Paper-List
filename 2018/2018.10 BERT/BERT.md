## 0. Abstract

- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê³ ì„±ëŠ¥ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ì¸ BERTë¥¼ ì†Œê°œí•œë‹¤.
- BERTê°€ ë¬´ì—‡ì´ë©° ë‹¤ë¥¸ ì„ë² ë”© ëª¨ë¸ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì´í•´í•˜ëŠ”ë°ì„œë¶€í„° ì‹œì‘í•œ ë‹¤ìŒ BERTì˜ ë™ì‘ ë°©ì‹ê³¼ êµ¬ì¡°ë¥¼ ì‚´í´ë³¸ë‹¤.
- ë§ˆìŠ¤í¬ ì–¸ì–´ëª¨ë¸ë§(MLM)ê³¼ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(NSP)ì´ë¼ëŠ” ë‘ ê°€ì§€ í…ŒìŠ¤í¬ ê¸°ë°˜ BERT ëª¨ë¸ì´ ì–´ë–»ê²Œ ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ì§€ ì•Œì•„ë³¸ë‹¤.
- í•˜ìœ„ë‹¨ì–´ í† í°í™” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.
- ì• ì„  ë‚´ìš©ë“¤ì„ ì´í•´í•˜ê³ , ë…¼ë¬¸ì„ ë‹¤ì‹œ ë³¸ë‹¤.

---

## 1. BERTì˜ ê¸°ë³¸ ì»¨ì…‰

### 1-1. BERTë€

**BERT(Bidirectional Encoder Representation from Transformer)**ëŠ” êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ **í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸**ì…ë‹ˆë‹¤.

NLP ë¶„ì•¼ì—ì„œ ë§ì€ ê¸°ì—¬ë¥¼ í•´ì™”ìœ¼ë©°, BERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

### 1-2. BERT's Usefulness

ì´ì „ì˜Â **word2vec**ê³¼ ê°™ì€ ë¬¸ë§¥ ë…ë¦½(context-free) ì„ë² ë”© ëª¨ë¸ì€ í•´ë‹¹ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ê¸° í˜ë“¤ì—ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§ŒÂ **BERT**ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ë¬¸ë§¥ ê¸°ë°˜(context-based) ì„ë² ë”© ëª¨ë¸ë¡œÂ **ë¬¸ë§¥ ì •ë³´**ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìˆ©ë‹ˆë‹¤.

![[Untitled.png]]

ìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´í•´ê°€ ì‰¬ìš¸ ê²ƒ ê°™ì§€ë§Œ ì•„ë˜ì˜ ì˜ˆì‹œë¥¼ í†µí•´ BERTë¥¼ ì¡°ê¸ˆ ë” ìì„¸í•˜ê²Œ ì»¨ì…‰ì„ ì´í•´í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

- ì¶”ê°€ ì˜ˆì‹œ
    
    ì•„ë˜ì™€ ê°™ì´ ë‘ ë¬¸ì¥ì´ ìˆë‹¤ê³  í•˜ì.
    
    > A: ë‚˜ëŠ” í•­êµ¬ì— ë“¤ëŸ¬ ë°°ë¥¼ íƒ”ë‹¤.
    
    > B: ë‚˜ëŠ” ì‚¬ê³¼ì™€ ë°°ë¥¼ ë¨¹ì—ˆë‹¤.
    
    ë‘ ë¬¸ì¥ì—ì„œ 'ë°°'ë¼ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ì„œë¡œ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
    
    BERTëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ë¬¸ë§¥ìƒ ì˜ë¯¸ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´,Â ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ ì—°ê²° ì‹œì¼œ ì´í•´í•©ë‹ˆë‹¤.
    
    ![https://blog.kakaocdn.net/dn/nSUtl/btstsQHE2Hp/eIeuptU4eJw5HHWsCL7VK1/img.png](https://blog.kakaocdn.net/dn/nSUtl/btstsQHE2Hp/eIeuptU4eJw5HHWsCL7VK1/img.png)
    
    A ì¼€ì´ìŠ¤ì˜ ê²½ìš°, í•­êµ¬ì™€ í•¨ê»˜ ì–¸ê¸‰ë˜ì—ˆê¸°ì— í•´ë‹¹ 'ë°°'ì˜ ì˜ë¯¸ëŠ” shipì´ë¼ëŠ” ì˜ë¯¸ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ë˜í•œ Bì¼€ì´ìŠ¤ì—ì„œëŠ” ì•„ë§ˆë„ ì‚¬ê³¼ë‚˜ ë¨¹ì—ˆë‹¤ì™€ ê´€ë ¨ì„±ì„ ê°€ì§€ê³  pearë¼ëŠ” ì˜ë¯¸ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì´ë ‡ê²Œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ë©´ **ë‹¤ì˜ì–´ ë° ë™ìŒì´ì˜ì–´**ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
    

---

## 2. BERTì˜ ë™ì‘ ë°©ì‹ ë° êµ¬ì¡°

### 2-1. BERTì˜ ì´ë¦„ ì•Œì•„ë³´ê¸°

- **B** â€” **B**idrectional
        - RNNì„ ê³µë¶€í•´ë³´ì…¨ë‹¤ë©´ ì–‘ë°©í–¥ RNNì„ ë– ì˜¬ë¦¬ì‹œë©´ ì‰½ìŠµë‹ˆë‹¤.
    - íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ëŠ” ì›ë˜ ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ì¥ì„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ë”°ë¼ì„œ BERTëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì–»ì€ ì–‘ë°©í–¥ ì¸ì½”ë” í‘œí˜„ì…ë‹ˆë‹¤.
- **ER** â€” **E**ncoder + **R**epresentation
    
    BERTëŠ” ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì¸ì½”ë”-ë””ì½”ë”ê°€ ìˆëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ê³¼ ë‹¬ë¦¬ ì¸ì½”ë”ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    ë¬¸ì¥ì„ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ì…ë ¥í•˜ê³ , ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ í‘œí˜„ ë²¡í„°ë¥¼ ì¶œë ¥ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
    
    ì¦‰, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ëŠ” BERTì˜ **í‘œí˜„ ë²¡í„°** ì…ë‹ˆë‹¤.
    
- from
    
- **T** â€” **T**ransformer
    
    - Introduction
        
        - SInce 2016,
            
            - In 2016, Google puplished thier nueral machine translation system(GNMT), which outperforms previous traditional MT system.
            
            [Googleâ€™s Neural Machine Translation System.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3acf089a-afaa-4cf8-8aee-c3a6cd364982/Googles_Neural_Machine_Translation_System.pdf)
            
            ![[Untitled 1.png]]
            
            
            - However, RNN based Sequence-to-sequence reveals its limitations.
            ![[Untitled 2.png]]

            
        - Fully Convilutional Seq2Seq[Gehring et al.2017p]
            
            [Convolutional Sequence to Sequence Learning.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e3cc7dc7-cf90-4146-8f2a-cf36742cd3a0/Convolutional_Sequence_to_Sequence_Learning.pdf)
            
            ![[Untitled 3.png]]
            
        - Attention is all you need
            
            [Attention Is All You Need.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/41b6edfe-428a-4fe2-bff7-7754f8dab242/Attention_Is_All_You_Need.pdf)
            
        - Transformer
            
            - Encoder + decoder
                
                ![[Untitled 4.png]]
                
            - ì„±ëŠ¥ê³¼ ì†ë„ ëª¨ë‘ ê¸°ì¡´ ëª¨ë¸ì„ ì••ë„
                
                ![[Untitled 5.png]]
                
        - Transformer and MLP
            
            - Pretraining and finetuning (Transfer Learning) with Big-LM.
            
            ![[Untitled 6.png]]
            
    - Multi-head Attention
        
        - Attention: Query Generation
            
            - Example
            
            ![[Untitled 6.png]]
            
            ![[Untitled 7.png]] ![[Untitled 8.png]]
            
            
            
            - ë§ˆìŒì˜ ìƒíƒœ(state)ë¥¼ ì˜ ë°˜ì˜í•˜ë©´ì„œ ì¢‹ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì´ëŒì–´ë‚´ëŠ” ì¿¼ë¦¬ë¥¼ ì–»ê¸° ìœ„í•¨
            - ë§Œì•½ ê²€ìƒ‰ì„ ë‹¤ì–‘í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤ë©´?
        - Transformer & Attention
            
            - Scaled Dot-product Attention
                
                ![[Untitled 9.png]]
                
                
            - Multi-Head Attention
                
                ![[Untitled 10.png]]
                
            - Transformer
                
                ![[Untitled 11.png]]
                
            - Encoder&Decoderê°€ ì—¬ëŸ¬ ì¸µì¸ ê²ƒì„ êµ³ì´ ê·¸ë ¤ë³´ìë©´
                
                ![[Untitled 12.png]]
                
        - Equations
            
            [Multihead_Attention.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2af6f0f6-515f-4b84-b7f9-b8eaf9a24092/Multihead_Attention.pdf)
            
            ![[Untitled 13.png]]
            
        - Summary
            
            - Previous method: attention in sequence to sequence
                - Queryë¥¼ ì˜ ë§Œë“¤ì–´ key-valueë¥¼ ì˜ matchingì‹œí‚¤ì
            - Multi-head Attention
                - ì—¬ëŸ¬ê°œì˜ Queryë¥¼ ë§Œë“¤ì–´ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ì˜ ì–»ì–´ì˜¤ì
            - Attention ìì²´ë¡œë„ ì •ë³´ì˜ encodingê³¼ decodingì´ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤Œ
    - Encoder block
        
        - Transformer
            
            ![[Untitled 14.png]]
            
            
        - Equations
            
            - Q,K and V are from previous alayer:
                
                - Residual connections and Layer Normalizations are used.
                    
                    [Deep Residual Learning for Image Recognition.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/94cae579-4b3d-4b43-8267-8c93d153fd63/Deep_Residual_Learning_for_Image_Recognition.pdf)
                    
                    [Layer Normalization.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cd879c33-928d-4973-a013-85d6964ecbc9/Layer_Normalization.pdf)
                    
                
                ![[Untitled 14.png]]
            - Encoder is stack of encoder blocks:
                
                ![[Untitled 15.png]]
                
        - Summary
            
            - EncoderëŠ” self-attentionìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ
                - Q,K,VëŠ” ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ - ì¦‰, ê°™ì€ ê°’
            - Seq2Seqì˜ Attentionê³¼ ë‹¬ë¦¬, Që„ ëª¨ë“  time-stepì„ ë™ì‹œì— ì—°ì‚°
                - ë¹ ë¥´ì§€ë§Œ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ë¨¹ê²Œ ë¨
            - Residual connectionìœ¼ë¡œ ì¸í•´ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ê°€ëŠ¥
                - Big LMì˜ í† ëŒ€ ë§ˆë ¨
    - Decoder with Masking
        
        - Transformer
            
            ![[Untitled 16.png]]
            
        - Equations
            
            - Given Dataset.
                
                ![[Untitled 17.png]]
                
            - What we want is
                
                ![[Untitled 18.png]]
                
            - Before we start
                
                - Using mask, assign -âˆ to make 0s for softmax results..
                
                ![[Untitled 19.png]]
                
            - Decoder Self-attention with mask
                
                - ëª¨ë“  attentionì—ëŠ” <pad>ì— ë§ˆìŠ¤í‚¹ì´ ë“¤ì–´ê°„ë‹¤.
                - ë‹¨, ë””ì½”ë”ì—ì„œëŠ” AuroRegressiveí•œ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´, ë‹¤ìŒìŠ¤í…ì„ ë³´ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë§ˆìŠ¤í‚¹ì„ í•¨ê»˜ í•´ì¤˜ì•¼ í•œë‹¤.
                
                ![[Untitled 20.png]]
                
            - Attention from encoder with mask for <pad>
                
                ![[Untitled 21.png]]
                
            - FC layers
                
                ![[Untitled 22.png]]
                
            - Decoder is stack of decoder blocks.
                
                ![[Untitled 23.png]]
                
            - Generator
                
                ![[Untitled 24.png]]
                
        - Summary
            
            - DecoderëŠ” 2ê°€ì§€ì˜ Attentionìœ¼ë¡œ êµ¬ì„±ë¨
                - Attention from encoder:
                    - Kì™€ VëŠ” encoderì˜ ìµœì¢… ì¶œë ¥ ê°’, QëŠ” ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ ê°’
                - Self-Attention with mask:
                    - Q,K,VëŠ” ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ ê°’
                    - Attention weight ê³„ì‚° ì‹œ, softmax ì—°ì‚° ì´ì „ì— maskingì„ í†µí•´ ìŒì˜ ë¬´í•œëŒ€ë¥¼ ì£¼ì–´, ë¯¸ë˜ time-stepì„ ë³´ëŠ” ê²ƒì„ ë°©ì§€
            - ì¶”ë¡  ë•Œì—ëŠ” self-attentionì˜ maskëŠ” í•„ìš” ì—†ìœ¼ë‚˜, ëª¨ë“  layerì˜ t ì‹œì  ì´ì „ì˜ ëª¨ë“  time-step(<t)ì˜ hidden_stateê°€ í•„ìš”
    - Positional Encoding
        
        - Unlike RNN,
            
            - TransformerëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ ìŠ¤ìŠ¤ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ(Conv2Së„ ë§ˆì°¬ê°€ì§€)
                - $h_{t}=f(x_{t},h_{t-1})$ â†’ RNN ê³„ì—´ì€ ìœ„ì¹˜ ì •ë³´ë¥¼ ìŠ¤ìŠ¤ë¡œ ì²˜ë¦¬í–ˆìŒ
                - ë§ˆì¹˜ FC layerì˜ ì…ë ¥ feature ìˆœì„œë¥¼ ë°”ê¿” í•™ìŠµí•´ë„ ì„±ëŠ¥ì´ ë˜‘ê°™ì€ ê²ƒê³¼ ê°™ìŒ
            - ì…ë ¥ ìˆœì„œë¥¼ ë°”ê¿” ë„£ìœ¼ë©´ ì¶œë ¥ë„ ìˆœì„œê°€ ë°”ë€ ì±„ ê°™ì€ ê°’ì´ ë‚˜ì˜¬ ê²ƒ
            - ë”°ë¼ì„œ ìœ„ì¹˜ìˆœì„œ ì •ë³´ë¥¼ ë”°ë¡œ ì¸ì½”ë”©í•´ì„œ ë„£ì–´ì¤˜ì•¼ í•¨
        - Positonal Encoding
            
            - ê¸°ì¡´ì˜ word embedding ê°’ì— positonal encoding ê°’ì„ ë”í•´ì¤Œ
            
            ![[Untitled 25.png]]
            
        - vs Positional Embedding â†’ í•™ìŠµë„ ê°€ëŠ¥
            
            - ì‚¬ì‹¤ ìœ„ì¹˜ ì •ë³´ë„ integer ê°’ì´ë¯€ë¡œ embedding layerë¥¼ í†µí•´ ì„ë² ë”© í•  ìˆ˜ ìˆìŒ
            - BERTì™€ ê°™ì€ ëª¨ë¸ì€ positional encoding ëŒ€ì‹ ì— positional embeddingì„ ì‚¬ìš©í•˜ê¸° ë„í•¨
        - Summary
            
            - RNNê³¼ ë‹¬ë¦¬, ìˆœì„œ(ìœ„ì¹˜) ì •ë³´ë¥¼ encodingí•´ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”
                - í•™ìŠµì´ ì•„ë‹Œ ë‹¨ìˆœ ê³„ì‚° í›„ encoding
            - í•™ìŠµì— ì˜í•´ ë‹¬ë¼ì§€ëŠ” ê°’ì´ ì•„ë‹ˆë¯€ë¡œ, í•œë²ˆë§Œ ê³„ì‚°í•´ ë†“ìœ¼ë©´ ë¨
        
        [transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)
        
    - Learning rate warm-up and linear decay
        
        - Previous Method
            
            **SGD+Gradient Clipping**
            
            - ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•
            
            $\theta \leftarrow \theta-\gamma \bigtriangledown_{\theta}L(\theta)$
            
            - Learning rateì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
            - í•™ìŠµ í›„ë°˜ë¶€ì— LR decay í•´ì£¼ê¸°ë„
            
            **Adam**
            
            - Adaptiveí•˜ê²Œ LRì„ ì¡°ì ˆ
            
            ![[Untitled 26.png]]
            
            - ì¼ë¶€ ê¹Šì€ ë„¤íŠ¸ì›Œí¬(e.g.Transformer)ì—ì„œ ì„œ ì„±ëŠ¥ì´ ë‚®ìŒ
                - ë¬¸ì œëŠ” ì§€ê¸ˆì€ Transformerì˜ ì„¸ìƒ
        - Warm-up and Linear Decay(Noam Decay)
            
            - Heuristic Methods
                - Control learning rate for Adam with hyper-params
            - í•™ìŠµ ì´ˆê¸° ë¶ˆì•ˆì •í•œ gradientë¥¼ í†µí•´ ì˜ëª»ëœ momentumì„ ê°–ëŠ” ê²ƒì„ ë°©ì§€
                - Residual Connectionì„ í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒ??
                - ëŒ€ì²´ë¡œ 5% ê·¼ì²˜
            
            ![[Untitled 27.png]]
            
            - ê²°êµ­ Trial&Errorë°©ì‹ìœ¼ë¡œ Hyper-parameter íŠœë‹ì„ í•´ì•¼í•¨
                - ê°€ì¥ í•µì‹¬ì€ #warm-up stepsì™€ #total iterations.
                - ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ hyper-parmas: init LR, batch size
            - ì‹¬ì§€ì–´ íŠœë‹ì— ë”°ë¼ SGD+Gradient Clippingì´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸°ë„ í•¨
        - Rectified Adam[Liu et al., 2020]
            
            [On The Variance of the Adaptive Learning Rate and Beyond.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e17bb8aa-84f6-49e5-bc33-d0768d7073f6/On_The_Variance_of_the_Adaptive_Learning_Rate_and_Beyond.pdf)
            
            - Adamì´ ì˜ ë™ì‘í•˜ì§€ ì•ŠëŠ” ì´ìœ (ê°€ì„¤)
            - Due to the lack of samples in the early stage, the adaptive learning rate has an undesirably large variance, which leads to suspicious/bad local optima. â€“ [Liu et al., 2020]
            
            ![[Untitled 28.png]]
            
            - Pytorch êµ¬í˜„
                
                [https://github.com/LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)
                
                - `$pip install torch-optimizer`
    - Appendix: Beyond the paper
        
        - Transformerì˜ ë‹¨ì 
            
            - í•™ìŠµì´ ê¹Œë‹¤ë¡­ë‹¤.
                
                - Bad local optimaì— ë¹ ì§€ê¸° ë§¤ìš° ì‰¬ì›€
                    
                - ê·¸ëŸ°ë° paperì—ì„œ ì´ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ
                    
                    -  warm-up step, learning rate
                    
                    ![[Untitled 29.png]]
                    
            - ì˜¤ì£½í•˜ë©´,
                
                [Training Tips for the Transformer Model.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/769d69f2-b212-4045-81c2-a19503c572f3/Training_Tips_for_the_Transformer_Model.pdf)
                
                [Transformers without Tears_Improving the Normalization of Self-Attention.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/65ab91ae-94b7-4221-8962-a77b9c90d160/Transformers_without_Tears_Improving_the_Normalization_of_Self-Attention.pdf)
                
                [On the Variance of the Adaptive Learning Rate and Beyond.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/efe85efb-8dcb-491c-984c-541a34675367/On_the_Variance_of_the_Adaptive_Learning_Rate_and_Beyond.pdf)
                
        - On Layer Normalization in Transformer Architecture[Xiong et al., 2020]
            
            [On Layer Normalization in Transformer Architecture.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/40f54878-9814-4339-843a-4c284da92f6d/On_Layer_Normalization_in_Transformer_Architecture.pdf)
            
            - Previous Work:
                
                - Use Noam decay(warm-up and linear decay)
                - Rectified Adam(RAdam)
            - Propose:
                
                - Layer Normì˜ ìœ„ì¹˜ì— ë”°ë¼ í•™ìŠµì´ ìˆ˜ì›”í•´ì§
                    - LNì´ gradientë¥¼ í‰íƒ„í•˜ê²Œ ë°”ê¾¸ëŠ” íš¨ê³¼
                
                ![[Untitled 30.png]]
                
                ![[Untitled 31.png]]
                
            - Evaluation Results
                
                ![[Untitled 32.png]]
                
        - Summary
            
            - Pre-Norm ë°©ì‹ì„ í†µí•´ warm-up ë° LR íŠœë‹ ì œê±° ê°€ëŠ¥
                - LR decayëŠ” ì—¬ì „íˆ í•„ìš”
            - ê·¸ ë°–ì—ë„ Layer Normì„ ëŒ€ì²´í•˜ê±°ë‚˜, weight initializationì„ í™œìš©í•˜ì—¬ ì¢€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ í™•ë³´í•  ìˆ˜ ìˆìŒ

### 2-2. ë™ì‘ ë°©ì‹

- ì´ì „ ì˜ˆì‹œ **ë‚˜ëŠ” í•­êµ¬ì— ë“¤ëŸ¬ ë°°ë¥¼ íƒ”ë‹¤**ë¼ëŠ” A ë¬¸ì¥ì„ íŠ¸ëœìŠ¤ í¬ë¨¸ì˜ ì¸ì½”ë”ì— ì…ë ¥ìœ¼ë¡œ ì œê³µí•˜ê³  ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë° ëŒ€í•œ ì„ë² ë”©ì„ ì¶œë ¥ìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.
- ì¸ì½”ë”ì— ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ì¸ì½”ë”ëŠ” ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•´ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì˜ ë¬¸ë§¥ì„ ì´í•´í•´ ë¬¸ì¥ì— ì‡ëŠ” ê° ë‹¨ì–´ì˜ ë¬¸ë§¥ í‘œí˜„ì„ ì¶œë ¥ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

![[Untitled 33.png]]

### 2-3. êµ¬ì¡°

BERTëŠ” í¬ê¸°ì— ë”°ë¼ ì•„ë˜ì˜ ë‘ ëª¨ë¸ë¡œ ë‚˜ë‰œë‹¤.

- `Bert-base`: OpenAI GPTì™€ ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§. GPTì™€ì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ ì„¤ê³„ë¨
- `Bert-large`: BERTì˜ ìµœëŒ€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§

![[Untitled 34.png]]

![[Untitled 35.png]]

- ëª¨ë“  taskì— ëŒ€í•´ SOTA ë‹¬ì„±
    
- BERT-largeê°€ ì¼ë°˜ì ìœ¼ë¡œ base ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨
    
- ì‚¬ì „í•™ìŠµ ë•ë¶„ì— ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ì‘ì•„ë„ ëª¨ë¸ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì •í™•ë„ê°€ ìƒìŠ¹
    
    - ì‚¬ì „í•™ìŠµì„ í•œë‹¤ëŠ” ê²ƒì€ Generalí•˜ê²Œ ì‚¬ëŒë“¤ì˜ ì–¸ì–´ì²´ê³„ë¥¼ í•™ìŠµí•œë‹¤ë¼ê³  ì´í•´í•˜ë©´ ì‰½ìŠµë‹ˆë‹¤.
    - ì¦‰, ë¬¸ë§¥ê³¼ í‘œí˜„ì„ ì‚¬ì „í•™ìŠµì„ í†µí•´ ë°°ì›Œ ë†“ê³ , Down stream taskë¥¼ ëª©ì ì— ë”°ë¼ íŒŒì¸íŠœë‹í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ê·¸ ë°–ì˜ ì—¬ëŸ¬ BERT ê¸°ë³¸ êµ¬ì¡°
    
    |BERT-tiny|L=2,A=2,H=128|
    |---|---|
    |BERT-mini|L=4,A=4,H=256|
    |BERT-small|L=4,A=8,H=521|
    |BERT-medium|L=8,A=8,H=521|
    

---

## 3. BERTì˜ ì‚¬ì „ í•™ìŠµ

### 3-1. BERTì˜ ì…ë ¥ í‘œí˜„

ì´ì œ ìš°ë¦¬ëŠ” BERTì— ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê¸° ì „ì— ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

ê·¸ë¦¬ê³  ì•„ë˜ì™€ ê°™ì´ 3ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

- Token Embedding (2)
    
    - Sentence PairëŠ” í•©ì³ì ¸ì„œ ë‹¨ì¼ Sequenceë¡œ ì…ë ¥ë˜ê³ , PairëŠ” í•œ ê°œ í˜¹ì€ 2ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.
        
        - e.g. Translation
            - 1: My dog is cute, he likes playing.
            - 2: ë‚˜ì˜ ê°•ì•„ì§€ëŠ” ê·€ì—½ê³ , ë…¸ëŠ” ê²ƒì„ ì¢‹ì•„í•œë‹¤.
    - ë¬¸ì¥ì˜ ì‹œì‘ ë¶€ë¶„ì—ëŠ” `[CLS]` ë¼ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        
        - [CLS] í† í°ì˜ ê²½ìš°, ë¶„ë¥˜ í…ŒìŠ¤í¬ì—ì„œë§Œ ì‚¬ìš©ë˜ì§€ë§Œ ë‹¤ë¥¸ í…ŒìŠ¤í¬ì—ì„œë„ ë°˜ë“œì‹œ ì¶”ê°€í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
    - ë¬¸ì¥ì˜ ëì—ëŠ” `[SEP]` ë¼ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        
    - ì˜ˆì‹œ
        
        ![[Untitled 36.png]]
        
        - ê¸°ì¡´ í† í°: `token = [my,dog,is,cute,he,likes,playing]`
        - í† í° ì„ë² ë”©ì„ ê±°ì¹  ê²½ìš°: `token_embedding=[[CLS], my, dog, is, cute, [SEP], he, likes, playing ,[SEP]]`
- Segment Embedding (3)
    
    - Segment Embeddingì€ ì£¼ì–´ì§„ ë‘ ë¬¸ì¥ì„ êµ¬ë³„í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
        
        - í† í° ì„ë² ë”©ì´ ì§„í–‰ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
            - `token_embedding=[[CLS], my, dog, is, cute, [SEP], he, likes, playing ,[SEP]]`
        - ì˜ˆì‹œ ë¬¸ì¥1: My dog is cute
        - ì˜ˆì‹œ ë¬¸ì¥2: He likes playing
    - Segment Embedding LayerëŠ” ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ìœ¼ë¡œ $E_A$ì™€ $E_B$ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        ![[Untitled 37.png]]
        
        - 1ë²ˆ ë¬¸ì¥ì— ì†í•  ê²½ìš° $E_A$ë¥¼ ë°˜í™˜
        - 2ë²ˆ ë¬¸ì¥ì— ì†í•  ê²½ìš° $E_B$ë¥¼ ë°˜í™˜
- Position Embedding (4)
    
    [vs Positional Embedding â†’ í•™ìŠµë„ ê°€ëŠ¥](https://www.notion.so/vs-Positional-Embedding-7d912d5c51834c1b9ee42445393553c6?pvs=21)
    
    - ì£¼ì˜ í•˜ì‹¤ ì ì€ Positional Encodingê³¼ëŠ” ì¡°ê¸ˆ ë‹¤ë¥¸ Positional Embeddingì€ ë‹¤ë¥¸ ê°œë…ì…ë‹ˆë‹¤.
    - Transformerì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•œë‹¤ë©´, ì–´ë–¤ ë°˜ë³µ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëª¨ë“  ë‹¨ì–´ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•¨ì„ ì•Œê³  ìˆì„ ê²ƒì…ë‹ˆë‹¤.
    - ì´ì— ë”°ë¼, ë‹¨ì–´ì˜ ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•œë‹¤.
    
    ![[Untitled 38.png]]
    
    - ìœ„ ê·¸ë¦¼ì´ ìµœì¢…ì ìœ¼ë¡œ BERT ëª¨ë¸ì— ë“¤ì–´ê°€ëŠ” INPUT ê°’ì´ ë©ë‹ˆë‹¤.
- WordPeice Tokenizer (1)
    
    - ê·¸ëŸ°ë° Token Embeddingì¸µì— ë“¤ì–´ê°ˆ Inputìœ¼ë¡œ ì‚¬ìš©ë  í† í°ì´ ì–´ë–»ê²Œ ìª¼ê°œì§€ëŠ”ì§€ ì•Œì•„ì•¼ê² ì£ ?
    - BERTëŠ” í•˜ìœ„ ë‹¨ì–´ í† í°í™” ì•Œê³ ë¦¬ì¦˜([4. í•˜ìœ„ ë‹¨ì–´ í† í°í™” ì•Œê³ ë¦¬ì¦˜](https://www.notion.so/4-eb353e9dc9064723b2045ec89933c2b0?pvs=21))ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
    - í•œê¸€ì„ ê¸°ì¤€ìœ¼ë¡œ í˜•íƒœì†Œë¥¼ ë¶„ì„í•˜ë“¯ì´, ì˜ì–´ì˜ ê²½ìš° `pretraining`ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ í† í°í™” í•´ë³´ê² ìŠµë‹ˆë‹¤.
        - pretraining = pre + train + ing
        - `tokens = [ pre, ##train, ##ing ]`
    - ê·¸ë ‡ë‹¤ë©´ ì™œ í•˜ìœ„ ë‹¨ì–´ë¡œ ìª¼ê°œëŠ” ê²ƒì¼ê¹Œìš”?
        - í•˜ìœ„ ë‹¨ì–´ë¡œ ìª¼ê°¤ ê²½ìš°, ì–´íœ˜ ì‚¬ì „ ì´ì™¸(OOV, Out-Of-Vocabulary)ì˜ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.
        - ê¸°ë³¸ì ìœ¼ë¡œ BERTì˜ ì–´íœ˜ ì‚¬ì „ì€ 3ë§Œ í† í°ì´ë¯€ë¡œ, ì™ ë§Œí•œ ì–´íœ˜ë“¤ì€ í† í°í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        - ë§Œì•½ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í† í°ì´ë¼ë©´ ì–´ë–»ê²Œë˜ëŠ”ì§€ëŠ” [4. í•˜ìœ„ ë‹¨ì–´ í† í°í™” ì•Œê³ ë¦¬ì¦˜](https://www.notion.so/4-eb353e9dc9064723b2045ec89933c2b0?pvs=21) ì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤.

### 3-2. ì‚¬ì „ í•™ìŠµ ì „ëµ

- ê¸°ì¡´ì˜ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ 
    
    ![[Untitled 39.png]]
    
    - ì „í†µì ì¸ ì–¸ì–´ ëª¨ë¸ë§(Language Modeling):Â **n-gram**, ì•ì˜ N-1ê°œì˜ ë‹¨ì–´ë¡œ ë’¤ì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸
    - í•„ì—°ì ìœ¼ë¡œ ë‹¨ë°©í–¥ì¼ìˆ˜ ë°–ì— ì—†ê³ , BiLMì„ ì‚¬ìš©í•˜ëŠ” ELMoë”ë¼ë„ ìˆœë°©í–¥, ìˆœë°©í–¥ì˜ ì–¸ì–´ ëª¨ë¸ì„ ë‘˜ ë‹¤ í•™ìŠµí•´ í™œìš©í•˜ì§€ë§Œ,
    - ë‹¨ë°©í–¥ ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì„ concatí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ì •ë„ì´ë¯€ë¡œ ì œí•œì ì¸ ì–‘ë°©í–¥ì„±ì„ ê°€ì§

---

**NEW**

- ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§(**MLM**, Masked Language Modeling)
    - MLMì€ ì¼ë°˜ì ìœ¼ë¡œ ì„ì˜ì˜ ë¬¸ì¥ì´ ì£¼ì–´ì§€ê³  ë‹¨ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ë³´ë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
        
    - MLMì˜ ì¢…ë¥˜ 2ê°€ì§€
        
        - ìê·€ íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§ â€” (ë‹¨ë°©í–¥)
            - ì• â†’ ë ë°©í–¥ìœ¼ë¡œ ì˜ˆì¸¡(ì „ë°© ì˜ˆì¸¡)
            - ë â†’ ì• ë°©í–¥ìœ¼ë¡œ ì˜ˆì¸¡(í›„ë°© ì˜ˆì¸¡)
            - ì˜ˆì‹œ
                
                - `Paris is a beautiful city. I love Paris.` ë¼ëŠ” ë‘ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤.
                
                1. ì²˜ìŒì—ëŠ” cityë¼ëŠ” ë‹¨ì–´ì— ê³µë°±ì„ ì¶”ê°€í•œë‹¤.
                    1. `Paris is a beautiful __. I love Paris.`
                2. ì´ì œ ëª¨ë¸ì€ ê³µë°±ì„ ì˜ˆì¸¡í•œë‹¤.
                    1. ì „ë°© ì˜ˆì¸¡ì€ Parisë¶€í„° ë¬¸ì¥ì„ ì½ëŠ”ë‹¤.
                        1. (â†’) `Paris is a beautiful __.`
                    2. í›„ë°© ì˜ˆì¸¡ì€ ëì˜ Parisë¶€í„° ë¬¸ì¥ì„ ì½ëŠ”ë‹¤.
                        1. (â†) `__. I love Paris.`
            - ìë™ íšŒê·€ ì–¸ì–´ ëª¨ë¸ì€ ì›ë˜ ë‹¨ ë°©í–¥ì´ë¯€ë¡œ í•œ ë°©í–¥ìœ¼ë¡œë§Œ ë¬¸ì¥ì„ ì½ìŠµë‹ˆë‹¤.
        - ìë™ ì¸ì½”ë”© ì–¸ì–´ ëª¨ë¸ë§ â€” (ì–‘ë°©í–¥)
            - ì´ ë°©ì‹ì˜ ê²½ìš°, ì „ë°© ë° í›„ë°© ì˜ˆì¸¡ì„ ëª¨ë‘ í™œìš©í•©ë‹ˆë‹¤.
            - ì¦‰, ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ì¥ì„ ì½ìŠµë‹ˆë‹¤.
            - (â†’)`Paris is a beautiful __. I love Paris.`(â†)
            - ë‹¹ì—°íˆ ë‹¨ë°©í–¥ ë³´ë‹¤ ë¬¸ì¥ ì´í•´ ì¸¡ë©´ì—ì„œ ë‚˜ìœ¼ë¯€ë¡œ, ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    - **BERTëŠ” ìë™ ì¸ì½”ë”© ì–¸ì–´ ëª¨ë¸ë¡œ, ì˜ˆì¸¡ì„ ìœ„í•´ ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ì¥ì„ ì½ìŠµë‹ˆë‹¤.**
        
    - **ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ì „ì²´ ë‹¨ì–´ì˜ 15%ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ê³ , ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.**
        
        - `Paris is a beautiful city. I love Paris.`
        - ìœ„ì˜ ì˜ˆì‹œì—ì„œ ë“¤ì—ˆë˜ ë¬¸ì¥ì„ í† í°í™” í•˜ê³  ë§ˆìŠ¤í‚¹í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
            - `tokens = [ [CLS], Paris, is, a, beatiful, [MASK], [SEP], I, love, Paris, [SEP] ]`
    - í•˜ì§€ë§Œ ìœ„ì™€ ê°™ì´ í† í°í™”ë¥¼ í•˜ê²Œ ë  ê²½ìš°, ì‚¬ì „í•™ìŠµê³¼ íŒŒì¸íŠœë‹ ì‚¬ì´ì—ì„œ ë¶ˆì¼ì¹˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. `[MASK]`í† í°ì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        
    - ì´ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ **80-10-10% ê·œì¹™**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        - ê¸°ì¡´ 15% ì¤‘ **80%**ì˜ í† í°ì„ [MASK] í† í°ìœ¼ë¡œ êµì²´í•œë‹¤.
            - `tokens = [ [CLS], Paris, is, a, beatiful, [MASK], [SEP], I, love, Paris, [SEP] ]`
        - 15% ì¤‘ **10%**ì˜ í† í°ì„ ì„ì˜ì˜ í† í°ìœ¼ë¡œ êµì²´í•œë‹¤.
            - `tokens = [ [CLS], Paris, is, a, beatiful, **love**, [SEP], I, love, Paris, [SEP] ]`
        - 15% ì¤‘ ë‚˜ë¨¸ì§€ 10%ì˜ í† í°ì€ ì–´ë–¤ ë³€ê²½ë„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
            - `tokens = [ [CLS], Paris, is, a, beatiful, city, [SEP], I, love, Paris, [SEP] ]`
    - ì´í›„, ì•ì„œ ì–¸ê¸‰ëœ (Token, Segment, Position) Embedding ì¸µì„ ê±°ì³ ì…ë ¥ ì„ë² ë”©(í† í°ì˜ í‘œí˜„ ë²¡í„°)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        - $R_{[CLS]}$ëŠ” [CLS] í† í°ì˜ í‘œí˜„ ë²¡í„°ë¥¼ ì˜ë¯¸í•˜ê³ , $R_{[Paris]}$ëŠ” Parisì˜ í‘œí˜„ ë²¡í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
        
        ![[Untitled 40.png]]
        
    - ì´ì œ ìš°ë¦¬ëŠ” í† í°ì˜ í‘œí˜„ ë²¡í„°ë¥¼ ì–»ì—ˆìœ¼ë¯€ë¡œ, ë§ˆìŠ¤í¬ëœ í† í°ì„ ì˜ˆì¸¡í•´ì•¼í•œë‹¤.
        
        - BERTì—ì„œ ë°˜í™˜ëœ ë§ˆìŠ¤í¬ëœ í† í° $R_{[MASK]}$ì˜ í‘œí˜„ì„ ($softmax$í™œì„±í™”+$feed-forward$) ë„¤í¬ì›Œí¬ì— ì…ë ¥í•œë‹¤.
        - ì´í›„, ìš°ë¦¬ëŠ” í•´ë‹¹ ë§ˆìŠ¤í¬ ìœ„ì¹˜ì˜ ë‹¨ì–´ê°€ ë  í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆê²Œ ëœë‹¤.
        
        ![[Untitled 41.png]]
        
    - ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹(WWM, Whole Word Masking)
        
        - MLMì—ì„œ ì¡°ê¸ˆ ë” ë‚˜ì•„ê°€ ì‹¬í™” ë‚´ìš©ì¸ ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì.
            - Let us start pretraining the modelì´ë¼ëŠ” ë¬¸ì¥ì„ ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ ë¬¸ì¥ì„ í† í°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í† í°ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
            - `tokens = [let, us, start, pre, ##train, ##ing, the, model]`
            - `tokens = [ [CLS], let, us, start, pre, ##train, ##ing, the, model, [SEP]]`
            - `tokens = [ [CLS], [MASK], us, start, pre, [MASK], ##ing, the, model, [SEP]]`
        - WWM ë°©ë²•ì—ì„œëŠ” í•˜ìœ„ ë‹¨ì–´ê°€ ë§ˆìŠ¤í‚¹ ë˜ë©´ ê´€ë ¨ëœ ëª¨ë“  ë‹¨ì–´ê°€ ë§ˆìŠ¤í‚¹ëœë‹¤.
            - `tokens = [ [CLS], [MASK], us, start, [MASK], [MASK], [MASK], the, model, [SEP]]`
        - ë§ˆìŠ¤í¬ ë¹„ìœ¨(15%)ë¥¼ ì´ˆê³¼í•˜ë©´ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ë§ˆìŠ¤í¬ë¥¼ ë¬´ì‹œí•œë‹¤. ì´ ë•ŒëŠ” letì„ ë¬´ì‹œí•œë‹¤.
            - `tokens = [ [CLS], let, us, start, [MASK], [MASK], [MASK], the, model, [SEP]]`
        - ì´í›„ëŠ” ë™ì¼í•˜ê²Œ ë§ˆìŠ¤í¬ ëœ í† í°ì„ í•™ìŠµí•˜ë„ë¡ í•œë‹¤.
- ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(**NSP**, Next Sentence Prediction)
    - NSPëŠ” BERT í•™ìŠµì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¤ë¥¸ í…ŒìŠ¤í¬ë¡œ, ì´ì§„ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ë‹¤.
        
        - isNext
        - NotNext
        
        |ë¬¸ì¥ ìŒ|ë ˆì´ë¸”|
        |---|---|
        |She cooked pasta||
        |It was delicious|isNext|
        |Birds fly in the sky.||
        |He was reading|NotNext|
        
    - NSPëŠ” ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ë©°, ì§ˆë¬¸-ì‘ë‹µ ë° ìœ ì‚¬ë¬¸ì¥íƒì§€ì™€ ê°™ì€ ë‹¤ìš´ ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤í¬ì—ì„œ ìœ ìš©í•˜ë‹¤.
        
    - ì˜ˆì‹œë¥¼ í†µí•´ ì‚´í´ë³´ì.
        
        ![[Untitled 42.png]]
        
    - BERTëŠ” [CLS] í† í°ë§Œ ê°€ì ¸ì™€ ë¶„ë¥˜ ì‘ì—…í•œë‹¤.
        
        - [CLS]í† í°ì€ ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  í† í°ì˜ ì§‘ê³„ í‘œí˜„ì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë¬¸ì¥ ì „ì²´ì— ëŒ€í•œ í‘œí˜„ì„ ë‹´ê³  ìˆë‹¤.
        - í•™ìŠµ ì´ˆê¸°ì—ëŠ” ë¬¼ë¡  í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë° ì¸ì½”ë” ê³„ì¸µì˜ ê°€ì¤‘ì¹˜ê°€ ìµœì ì´ ì•„ë‹ˆë¼ ì˜¬ë°”ë¥¸ í™•ë¥ ì„ ë°˜í™˜í•˜ì§€ ëª»í•˜ì§€ë§Œ,
        - ì—­ì „íŒŒë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°˜ë³µ í•™ìŠµì„ í†µí•´ ìµœì ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ê²Œ ë˜ë©´ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ ë°˜í™˜ ê°’ì„ ë‚´ë†“ê²Œ ëœë‹¤.
        
        ![[Untitled 43.png]]
        

---

### 3-3. ì‚¬ì „ í•™ìŠµ ì ˆì°¨

1. ë§ë­‰ì¹˜ì—ì„œ ë‘ ë¬¸ì¥ A, Bë¥¼ ìƒ˜í”Œë§í•œë‹¤.
    - Aì™€ Bì˜ ì´ í† í° ìˆ˜ì˜ í•©ì€ 512ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì•„ì•¼ í•œë‹¤.
    - ì „ì²´ì˜ 50%ì€ B ë¬¸ì¥ì´ A ë¬¸ì¥ê³¼ ì´ì–´ì§€ëŠ” ë¬¸ì¥(`IsNext`)ì´ ë˜ë„ë¡ ìƒ˜í”Œë§í•˜ê³ , ë‚˜ë¨¸ì§€ 50%ì€ B ë¬¸ì¥ì´ A ë¬¸ì¥ì˜ í›„ì† ë¬¸ì¥ì´ ì•„ë‹Œ ê²ƒ(`NotNext`)ìœ¼ë¡œ ìƒ˜í”Œë§í•œë‹¤.
2. ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ì„ í† í°í™”í•˜ê³ , í† í° ì„ë² ë”©-ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©-ìœ„ì¹˜ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê±°ì¹œë‹¤.
    - ì‹œì‘ ë¶€ë¶„ì—Â `[CLS]`Â í† í°ì„, ë¬¸ì¥ ëì—Â `[SEP]`Â í† í°ì„ ì¶”ê°€í•œë‹¤.
    - `80-10-10%`Â ê·œì¹™ì— ë”°ë¼ í† í°ì˜ 15%ë¥¼ ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹í•œë‹¤.
3. BERTì— í† í°ì„ ì…ë ¥í•˜ê³ , MLMê³¼ NSP íƒœìŠ¤í¬ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤
    - WarmUp Step(= 1ë§Œ): ì´ˆê¸° 1ë§Œ ìŠ¤í…ì€ í•™ìŠµë¥ ì´ 0ì—ì„œ 1e - 4ë¡œ ì„ í˜• ì¦ê°€, 1ë§Œ ìŠ¤í… ì´í›„ ì„ í˜• ê°ì†Œ
        
    - DropOut(0.1)
        
    - **GeLU Activation Func**Â : ìŒìˆ˜ì— ëŒ€í•´ì„œë„ ë¯¸ë¶„ì´ ê°€ëŠ¥í•´ ì•½ê°„ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŒ
        
        - GELUëŠ” ê°€ìš°ì‹œì•ˆ ì˜¤ì°¨ ì„ í˜• ìœ ë‹›(Gaussian Error Linear Unit)ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤.
        - $G E L U(x)=x \Phi(x)$
        - $\Phi$ì€ í‘œì¤€ ê°€ìš°ì‹œì•ˆ ëˆ„ì  ë¶„í¬ì´ë©°, GELUí•¨ìˆ˜ëŠ” ë‹¤ìŒ ìˆ˜ì‹ì˜ ê·¼ì‚¬ì¹˜ë¼ê³  í•©ë‹ˆë‹¤.
        - $\operatorname{GELU}(x)=0.5 x\left(1+\tanh \left[\sqrt{\frac{2}{\pi}}\left(\mathrm{x}+0.044715 x^3\right)\right]\right)$
        
        
        ![[Untitled 44.png]]

---

## 4. í•˜ìœ„ ë‹¨ì–´ í† í°í™” ì•Œê³ ë¦¬ì¦˜

### 4-1. í•˜ìœ„ ë‹¨ì–´ í† í°í™”ë¥¼ í•˜ëŠ” ì´ìœ 

- BERT ë° GPT-3ë¥¼ í¬í•¨í•œ ë§ì€ ìµœì‹  LLM ëª¨ë¸ì—ì„œëŠ” í•˜ìœ„ ë‹¨ì–´ í† í°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ê·¸ ì´ìœ ëŠ” ë°”ë¡œ OOVë‹¨ì–´ ì²˜ë¦¬ì— ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.
    - **OoV(**Out of Vocabulary): ë‹¨ì–´ ì§‘í•©ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì´ ìƒê¸°ëŠ” ìƒí™© (Inferenceì‹œ TrainSetì— ì—†ë˜ ë‹¨ì–´ê°€ ìˆì„ ê²½ìš°)
- OOVê°€ ë°œìƒí•œë‹¤ë©´ ì–´ë–»ê²Œ í• ê¹Œ?
    - OoV ë‹¨ì–´ ë°œìƒ ì‹œ `<UNK>` í† í°ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , `<UNK>` í† í°ì€ ëª¨ë¸ í•™ìŠµì— ìˆì–´ ë§¤ìš° ì¹˜ëª…ì ìœ¼ë¡œ ì‘ìš©í•©ë‹ˆë‹¤.
    - í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ Vocab ìì²´ê°€ ë§¤ìš° í¬ê¸° ë•Œë¬¸ì— ì‹ ì¡°ì–´ê°€ ì•„ë‹Œ ì´ìƒ ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ëŠ” í•˜ìœ„ ë‹¨ì–´ë¡œ í† í°í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4-2. ë°”ì´íŠ¸ ìŒ ì¸ì½”ë”©(BPE)

- **BPE**ì€ ë¹ˆë„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ìŒì˜ ë¬¸ìë‚˜ ë¬¸ìì—´ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
- ì´ ë°©ë²•ì€ ì´ˆê¸°ì— ë°ì´í„° ì••ì¶•ì— ì‚¬ìš©ë˜ì—ˆìœ¼ë‚˜, ë‚˜ì¤‘ì— ìì—°ì–´ ì²˜ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ í† í°í™”ì—ë„ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

1. Datasetì—ì„œ ëª¨ë“  ë‹¨ì–´ë¥¼ ë¹ˆë„ìˆ˜ì™€ í•¨ê»˜ ì¶”ì¶œ
2. ëª¨ë“  ë‹¨ì–´ë¥¼ ë¬¸ìë¡œ ë‚˜ëˆ„ê³  ë¬¸ì ì‹œí€€ìŠ¤ë¡œ ë§Œë“ ë‹¤.
3. ì–´íœ˜ ì‚¬ì „ í¬ê¸°ë¥¼ ì •ì˜í•œë‹¤.
4. ë¬¸ì ì‹œí€€ìŠ¤ì— ìˆëŠ” ëª¨ë“  ê³ ìœ ë¬¸ìë¥¼ ì–´íœ˜ ì‚¬ì „ì— ì¶”ê°€í•œë‹¤.
5. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ í° ê¸°í˜¸ ìŒì„ ì‹ë³„í•˜ê³ , í•´ë‹¹ ìŒì„ ë³‘í•©í•´ì„œ ì–´íœ˜ ì‚¬ì „ì— ì¶”ê°€í•œë‹¤.
6. ì–´íœ˜ ì‚¬ì „ í¬ê¸°ì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ 5ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.

- ì°¸ê³  ë§í¬
    
    [Byte pair encoding ì„¤ëª… (BPE tokenizer, BPE ì„¤ëª…, BPE ì˜ˆì‹œ)](https://process-mining.tistory.com/189)
    

### 4-3. ë°”ì´íŠ¸ ìˆ˜ì¤€ ë°”ì´íŠ¸ ìŒ ì¸ì½”ë”©(BBPE)

- BPEì™€ ë™ì‘ ë°©ì‹ì´ ê±°ì˜ ìœ ì‚¬í•˜ì§€ë§Œ, ë‹¨ì–´ë¥¼ ë¬¸ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ë°”ì´íŠ¸ ìˆ˜ì¤€ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- ì´ë¥¼ í†µí•´, ë‹¤êµ­ì–´ì— ëŒ€í•´ ì–´íœ˜ ì‚¬ì „ì„ ê³µìœ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

1. Datasetì—ì„œ ëª¨ë“  ë‹¨ì–´ë¥¼ ë¹ˆë„ìˆ˜ì™€ í•¨ê»˜ ì¶”ì¶œ
2. ~~ëª¨ë“  ë‹¨ì–´ë¥¼ ë¬¸ìë¡œ ë‚˜ëˆ„ê³  ë¬¸ì ì‹œí€€ìŠ¤ë¡œ ë§Œë“ ë‹¤.~~ â†’ ëª¨ë“  ë‹¨ì–´ë¥¼ ë¬¸ìë¡œ ë‚˜ëˆ„ê³  ë°”ì´íŠ¸ ìˆ˜ì¤€ ì‹œí€€ìŠ¤ ë§Œë“ ë‹¤.
3. ì–´íœ˜ ì‚¬ì „ í¬ê¸°ë¥¼ ì •ì˜í•œë‹¤.
4. ë¬¸ì ì‹œí€€ìŠ¤ì— ìˆëŠ” ëª¨ë“  ê³ ìœ ë¬¸ìë¥¼ ì–´íœ˜ ì‚¬ì „ì— ì¶”ê°€í•œë‹¤.
5. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ í° ê¸°í˜¸ ìŒì„ ì‹ë³„í•˜ê³ , í•´ë‹¹ ìŒì„ ë³‘í•©í•´ì„œ ì–´íœ˜ ì‚¬ì „ì— ì¶”ê°€í•œë‹¤.
6. ì–´íœ˜ ì‚¬ì „ í¬ê¸°ì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ 5ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.

### 4-4. ì›Œë“œ í”¼ìŠ¤

- ì›Œë“œ í”¼ìŠ¤ëŠ” BPEì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•˜ì§€ë§Œ, í•œ ê°€ì§€ ì°¨ì´ì ì´ ìˆë‹¤.
- BPEì—ì„œëŠ” ë°ì´í„°ì…‹ì—ì„œ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ì¶”ì¶œí•˜ê³  ë‹¨ì–´ë¥¼ ë¬¸ì ì‹œí€€ìŠ¤ë¡œ ë‚˜ëˆˆë‹¤. ì´í›„, ì–´íœ˜ ì‚¬ì „ í¬ê¸°ì— ë„ë‹¬í•  ë•Œ ê¹Œì§€ ê³ ë¹ˆë„ ê¸°í˜¸ ìŒì„ ë³‘í•©í•œë‹¤.
- í•˜ì§€ë§Œ ì›Œë“œí”¼ìŠ¤ëŠ” ì‹¬ë³¼ ìŒì„ ë³‘í•©í•˜ì§€ ì•Šê³ , ê°€ëŠ¥ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•œë‹¤.
- ì‹¬ë³¼ìŒì˜ ê°€ëŠ¥ë„ë¥¼ ê³„ì‚°
    - $argmax(\frac{p(st)}{p(s)p(t)})$

---

## 5. Code Review

### Transformer(ì§ì ‘ êµ¬í˜„)

[](https://github.com/dorae222/DeepLearning/blob/main/4.%20Lv4_NLG/simple_nmt/models/transformer.py)

### BERT

- ì½”ë“œë¥¼ ìì„¸í•˜ê²Œ ë¶„ì„í•˜ê³  ì‘ì„±í•˜ê¸°ì— ì‹œê°„ì´ ë¶€ì¡±í•˜ì—¬, ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ì½”ë“œë¥¼ ë§¤ì¹­ ì‹œì¼œ ì´í•´í•œëŒ€ë¡œ í¸ì§‘í•˜ì˜€ìŠµë‹ˆë‹¤.
- [ì½”ë“œ ë§í¬](https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891)
- Full Architecture
    - í† í°í™”
    - PositionalEmbedding
    - Transformer Encoder
    - BERT(MLM + NSP)

---

- í† í°í™”
    
    ![[Untitled 45.png]]
    
- PositionalEmbedding
    
    ![[Untitled 46.png]]
    
- Transformer Encoder
    
    ![[Untitled 47.png]]
    
- BERT(MLM + NSP)
    
    ![[Untitled 48.png]]
    
    ![[Untitled 49.png]]
    
    ![[Untitled 50.png]]
    
    ![[Untitled 51.png]]
    

---

- FineTuning ì˜ˆì‹œ ì½”ë“œ(êµë‚´ ë™ì•„ë¦¬ì—ì„œ ì§„í–‰ëœ ë°©ì–¸ ë¶„ë¥˜ ì½”ë“œì…ë‹ˆë‹¤.)
    
    [](https://github.com/dorae222/HAI_Kaggle_Competition/blob/main/v_1_hai_kaggle_summer.ipynb)
    

---

## 6. ë…¼ë¬¸ ë¦¬ë·°

<aside> ğŸ’¡ ì•ì˜ 1~5ê¹Œì§€ ë‚´ìš©ì„ ë³´ì‹œê³  ì•„ë˜ ë‚´ìš©ì„ ë³´ì‹œë©´ ì•„ë˜ ë…¼ë¬¸ì„ ì´í•´í•˜ê¸° ì‰¬ìš¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

</aside>

[BERT: Pre-training of Deep Bidirectional Transformers for Language...](https://arxiv.org/abs/1810.04805)

### **1. Introduction (ì„œë¡ )**

- BERT(Bidirectional Encoder Representations from Transformers)ëŠ” ì–‘ë°©í–¥ Transformer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì…ë‹ˆë‹¤.
- ì‚¬ì „ í•™ìŠµ(pre-training)ê³¼ ë¯¸ì„¸ ì¡°ì •(fine-tuning)ì˜ ë‘ ë‹¨ê³„ë¡œ í•™ìŠµë˜ë©°, ë‹¤ì–‘í•œ NLP ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **2. Related Work (ê´€ë ¨ ì—°êµ¬)**

### 2.1 Unsupervised Feature-based Approaches

- Word2Vec, GloVeì™€ ê°™ì€ ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning) ë°©ë²•ì„ í†µí•´ ë‹¨ì–´ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ ì–¸ê¸‰ë©ë‹ˆë‹¤.
- ì´ëŸ¬í•œ ë°©ë²•ì€ ê° ë‹¨ì–´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì„ë² ë”©í•˜ëŠ”ë°, ë¬¸ë§¥ ì •ë³´ê°€ ì œëŒ€ë¡œ ë°˜ì˜ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
- **ë‹¨ì–´ í˜¹ì€ ë¬¸ì¥ì˜ representation í•™ìŠµ**
    - non-neural method
        
        [Class-Based n-gram Models of Natural Language](https://aclanthology.org/J92-4003/)
        
        [](https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf)
        
        [Domain Adaptation with Structural Correspondence Learning](https://aclanthology.org/W06-1615/)
        
    - neural method
        
        [One Billion Word Benchmark for Measuring Progress in Statistical...](https://arxiv.org/abs/1312.3005)
        
        [](https://nlp.stanford.edu/pubs/glove.pdf)
        

### 2.2 Unsupervised Fine-tuning Approaches

- ELMo, GPTì™€ ê°™ì´ ë¯¸ë¦¬ í° ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµì„ ì‹œí‚¨ í›„, íŠ¹ì • ì‘ì—…ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •(fine-tuning)ì„ í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. GPTëŠ” íŠ¹íˆ Transformerì˜ ë””ì½”ë”ë¥¼ ì‚¬ìš©í•´ ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ì£¼ë¡œ ë‹¨ë°©í–¥(ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ë˜ëŠ” ê·¸ ë°˜ëŒ€) ì •ë³´ë§Œì„ ê³ ë ¤í•©ë‹ˆë‹¤.

### 2.3 Transfer Learning from Supervised Data

- ì§€ë„ í•™ìŠµ(supervised learning)ì—ì„œ ì–»ì€ ì§€ì‹ì„ ë‹¤ë¥¸ ì‘ì—…ì— ì ìš©í•˜ëŠ” ì „ì´ í•™ìŠµ(transfer learning)ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ë¥¸ NLP ì‘ì—…ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ° ë°©ë²•ì€ í•­ìƒ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.

### **3. BERT (ëª¨ë¸ ì„¤ëª…)**

![[Untitled 52.png]]

### 3.1 Pre-training BERT

- **ë°ì´í„° ë° ì•„í‚¤í…ì²˜**: BERTëŠ” ëŒ€ê·œëª¨ì˜ ì–¸ì–´ ë°ì´í„°(ì˜ˆ: Wikipedia)ë¥¼ ì‚¬ìš©í•˜ì—¬ Transformerì˜ ì¸ì½”ë” êµ¬ì¡°ë¥¼ ì‚¬ì „ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë•Œ ì•„í‚¤í…ì²˜ëŠ” ë‹¤ì–‘í•œ í¬ê¸°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ëŒ€í‘œì ìœ¼ë¡œ BERT-Baseì™€ BERT-Largeê°€ ìˆìŠµë‹ˆë‹¤.
- **Masked Language Model (MLM)**: ì „í†µì ì¸ ì–¸ì–´ ëª¨ë¸ì€ ë‹¨ë°©í–¥(ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ë˜ëŠ” ê·¸ ë°˜ëŒ€)ë§Œì„ ê³ ë ¤í•©ë‹ˆë‹¤. BERTëŠ” ì–‘ë°©í–¥ ì •ë³´ë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ MLM ì‘ì—…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì…ë ¥ ë¬¸ì¥ì—ì„œ ì¼ë¶€ ë‹¨ì–´ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ë¦° ë’¤(hidden or 'masked') ì´ ê°€ë ¤ì§„ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.
- **Next Sentence Prediction (NSP)**: ë‘ ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‘ ë²ˆì§¸ ë¬¸ì¥ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ ë‹¤ìŒì— ì˜¤ëŠ” ë¬¸ì¥ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ë¬¸ì¥ ê°„ì˜ ê´€ê³„ë¥¼ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3.2 Fine-tuning BERT

- **ì‘ì—… íŠ¹í™”**: ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì„ íŠ¹ì • NLP ì‘ì—…ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ë¬¸ì¥ ë¶„ë¥˜, ê°œì²´ ëª…ëª…, ì§ˆë¬¸ ì‘ë‹µ ë“±ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ë°ì´í„° ë° í•™ìŠµ**: ë¯¸ì„¸ ì¡°ì •ì€ ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. BERTì˜ Transformer ì¸ì½”ë”ëŠ” ê° ì‘ì—…ì˜ íŠ¹ì„±ì„ ìº¡ì²˜í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.
- **ì–‘ë°©í–¥ì„±ì˜ ì´ì **: ê¸°ì¡´ ëª¨ë¸ë“¤ì´ ì£¼ë¡œ ë‹¨ë°©í–¥ ì •ë³´ë§Œì„ í™œìš©í–ˆë‹¤ë©´, BERTëŠ” ì–‘ë°©í–¥ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ë” ì •í™•í•˜ê²Œ íŒŒì•…í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ë§¥ ì •ë³´ëŠ” íŠ¹íˆ ì˜ë¯¸ê°€ ì• ë§¤í•œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì—ì„œ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

### 4**. Experimental Results (ì‹¤í—˜ ê²°ê³¼)**

- ì €ë„ ì •ë¦¬ë¥¼ í•˜ë©´ì„œ ì°¾ì€ ë‚´ìš©ì¸ë°, Downstream tasksì— ê´€í•´ ì˜ ì •ë¦¬ëœ ë‚´ìš©ì´ ìˆì–´ ì²¨ë¶€í•©ë‹ˆë‹¤.
    
    [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://thejb.ai/bert/)
    

### 4.1 GLUE

- GLUEëŠ”Â General Language Understanding Evaluationì˜ ì•½ìë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ general language understanding taskë¥¼ í¬í•¨í•˜ê³  ì´ë¥¼ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤
- ì—¬ê¸°ì—ëŠ” ë¬¸ì¥ ë¶„ë¥˜, ê°œì²´ ëª…ëª… ë“± ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

![[Untitled 53.png]]

- Table 1
    
    ![[Untitled 54.png]]
    
    - ì‹¤í—˜ ê²°ê³¼ BERT Base,Large ëª¨ë‘ ê¸°ì¡´ì˜ ë°©ë²•ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤.
        
    - í˜¹ì‹œ ë²¤ì¹˜ë§ˆí¬ê°€ ê° ì–´ë–¤ í…ŒìŠ¤í¬ì¸ì§€ ëª¨ë¥´ì‹¤ ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ì•„ ì²¨ë¶€í•©ë‹ˆë‹¤.
        
        [NLP ì´í•´í•˜ê¸°](https://hryang06.github.io/nlp/NLP/)
        

### 4.2 SQuAD v1.1

- SQuAD v1.1(Stanford Question Answering Dataset)ì—ì„œì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- ì…ë ¥ : Context / question ìŒì˜ í˜•íƒœë¡œ ì œê³µë˜ë©°
- ì¶œë ¥ : Answer, ì •ìˆ˜ ìŒìœ¼ë¡œ, Context ë‚´ì— í¬í•¨ëœ ë‹µë³€ Textì˜ ì‹œì‘ê³¼ ëì„ ìƒ‰ì¸í™” í•©ë‹ˆë‹¤.
- ì£¼ë¡œ Exact Match(EM)ê³¼ F1 score ì§€í‘œë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ë©°, BERTì™€ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

![[Untitled 55.png]]

- Table 2
    
    ![[Untitled 56.png]]
    
- Table 3
    
    ![[Untitled 57.png]]
    

### 4.3 SQuAD v2.0

- SQuADëŠ” ì—¬ëŸ¬ ì†ŒìŠ¤ë“¤ì—ì„œ ëª¨ì¸ ì§ˆë¬¸/ë‹µë³€ ìŒ ë°ì´í„°ì´ë‹¤. í•˜ë‚˜ì˜ ì§ˆë¬¸ê³¼, ê·¸ì— ëŒ€í•œ ë‹µë³€ ë¬¸ë‹¨ì´ ë“¤ì–´ìˆê³ , ë‹µë³€ ë¬¸ë‹¨ ì¤‘ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ë˜ëŠ” êµ¬ë¬¸ì„ ì°¾ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.

![[Untitled 58.png]]

- Table 4
    
    ![[Untitled 59.png]]
    

### 4.4 SWAG

- SWAG(Situation With Adversarial Generations)ëŠ” ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ë…¼ë¦¬ì  ì¶”ë¡ ì„ í•˜ëŠ” ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
- ì´ í…Œì´ë¸”ì—ì„œëŠ” BERTê°€ ì–¼ë§ˆë‚˜ ì˜ ì¶”ë¡ ì„ í•˜ëŠ”ì§€ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.

![[Untitled 60.png]]

- Table 5
    
    ![[Untitled 61.png]]
    
    - **No NSP**: MLM ì‚¬ìš© / NSP ë¯¸ì‚¬ìš©
    - **LTR & No NSP**: MLM ëŒ€ì‹  Left-to-Right ì‚¬ìš© / NLP ë¯¸ì‚¬ìš©
    
    ---
    
    - NSP íƒœìŠ¤í¬ë¥¼ ì§„í–‰í•˜ì§€ ì•Šìœ¼ë©´ ìì—°ì–´ ì¶”ë¡  íƒœìŠ¤í¬(QNLI, MNLI)ì™€ QA íƒœìŠ¤í¬(SQuAD)ì—ì„œ í° ì„±ëŠ¥ í•˜ë½ì´ ìˆìŒ
    - MLM ëŒ€ì‹  LTRì´ë‚˜ BiLSTMì„ ì‚¬ìš©í–ˆì„ ë•Œ MRPCì™€ SQuADì—ì„œì˜ ì„±ëŠ¥ì´ í¬ê²Œ í•˜ë½í•¨. MLMì´ LTRê³¼ BiLSTMë³´ë‹¤ í›¨ì”¬ ê¹Šì€ ì–‘ë°©í–¥ì„±ì„ ëˆë‹¤.

### 5**. Ablation Studies (ì„±ëŠ¥ ë¶„ì„)**

### 5.1 Effect of Pre-training Tasks

- ì‚¬ì „ í›ˆë ¨ ì‘ì—…ì˜ ì¢…ë¥˜ê°€ ëª¨ë¸ ì„±ëŠ¥ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- ì˜ˆë¥¼ ë“¤ì–´, Masked Language Modeling(MLM)ê³¼ Next Sentence Prediction(NSP) ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ ì´ìš©í•´ ì–´ë–¤ ê²ƒì´ ë” ìœ ìš©í•œì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

### 5.2 Effect of Model Size

- ëª¨ë¸ í¬ê¸° (ì˜ˆ: ë ˆì´ì–´ ìˆ˜, íŒŒë¼ë¯¸í„° ìˆ˜ ë“±)ê°€ ì„±ëŠ¥ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
- ì´ë¥¼ í†µí•´ ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ í–¥ìƒë˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì–¸ì œ ê·¸ ì„±ëŠ¥ì´ ë” ì´ìƒ í–¥ìƒë˜ì§€ ì•ŠëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 5.3 Feature-based Approach with BERT

- BERTë¥¼ feature-based ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ê²½ìš° ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.
    
- ì˜ˆë¥¼ ë“¤ì–´, BERTì˜ ì¶œë ¥ì„ ë‹¤ë¥¸ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì™€ BERTë¥¼ end-to-endë¡œ í›ˆë ¨ì‹œí‚¤ëŠ” ê²½ìš°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
    
- Table 6
    

![[Untitled 62.png]]

- Table 7

![[Untitled 63.png]]

### 6**. Conclusion (ê²°ë¡ )**

- BERTëŠ” ì‚¬ì „ í•™ìŠµê³¼ ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ë‹¤ì–‘í•œ NLP ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©°,
- ì´ë¥¼ í†µí•´ ìƒˆë¡œìš´ ì—°êµ¬ ë° ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°€ëŠ¥ì„±ì´ í™•ëŒ€ë  ê²ƒì´ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦½ë‹ˆë‹¤.

---

## 7. ì¶”í›„ ë°©í–¥ì„±

- ì¶”ê°€ ì •ë¦¬ ëª©í‘œ
    - ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ ì¶”ê°€ íƒìƒ‰([10. BERT ì´í›„ ê´€ë ¨ ë…¼ë¬¸](https://www.notion.so/10-BERT-84c7fcdb22914ab892d65c6ab3d0715b?pvs=21))
    - ì‚¬ì „ í•™ìŠµëœ BERTì—ì„œ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•
    - BERTì˜ ëª¨ë“  ì¸ì½”ë” ë ˆì´ì–´ì—ì„œ ì„ë² ë”©ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•
    - ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ BERT íŒŒì¸ íŠœë‹ ë°©ë²•
    - ë‹¤ë¥¸ ì–¸ì–´ì— ì ìš©í•˜ëŠ” ë²•

---

## 8. ì°¸ê³  ë§í¬

- ë¦¬ìŠ¤íŠ¸
    - ì•„ë˜ ë§í¬ë“¤ì„ ì£¼ìš” ë‚´ìš©ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‚´ìš©ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
        
        [Word2vec vs BERT](https://medium.com/@ankiit/word2vec-vs-bert-d04ab3ade4c9)
        
        [GitHub - kh-kim/nlp_with_pytorch_examples: ë„ì„œ ë‚´ì˜ ì½”ë“œë“¤ì„ ëª¨ì•„ ë†“ì€ repoì…ë‹ˆë‹¤.](https://github.com/kh-kim/nlp_with_pytorch_examples)
        
        [GitHub - PacktPublishing/Getting-Started-with-Google-BERT: Getting Started with Google BERT, published by Packt](https://github.com/PacktPublishing/Getting-Started-with-Google-BERT)
        

---

## 9. BERT ì´ì „ì— ì°¸ê³ í•  ë§Œí•œ ë…¼ë¬¸

- ë¦¬ìŠ¤íŠ¸
    - [Semi-supervised sequence tagging with bidirectional language models. : ELMo](https://arxiv.org/abs/1705.00108)
    - [Attention is all you need. : Transformer](https://arxiv.org/abs/1706.03762)
    - [Class-based n-gram models of natural language : non-neural word representation (1)](https://aclanthology.org/J92-4003/)
    - [A framework for learning predictive structures from multiple tasks and unlabeled data : non-neural word representation (2)](https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf)
    - [Domain adaptation with structural correspondence learning. : non-neural word representation (3)](https://aclanthology.org/W06-1615.pdf)
    - [Distributed representations of words and phrases and their compositionality : neural word representation (1) (word2vec)](https://arxiv.org/abs/1310.4546)
    - [Glove: Global vectors for word representation. : neural word representation (2) (GloVe)](https://nlp.stanford.edu/pubs/glove.pdf)
    - [Semi-supervised sequence learning. : fine-tuning representation (1)](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/44267.pdf)
    - [Universal language model fine-tuning for text classification : fine-tuning representation (2)](https://arxiv.org/abs/1801.06146)
    - [Improving language understanding with Generative Pre-Training. : fine-tuning representation (3) (GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    - [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data : NLI ë¥¼ ì´ìš©í•´ì„œ í•™ìŠµ](https://arxiv.org/abs/1705.02364)
    - [Learned in translation: Contextualized word vectors. : MTë¥¼ ì´ìš©í•´ì„œ í•™ìŠµ (CoVe)](https://arxiv.org/abs/1708.00107)
    - [Googleâ€™s neural machine translation system: Bridging the gap between human and machine translation. : WordPiece embedding](https://arxiv.org/abs/1609.08144)

---

## 10. BERT ì´í›„ ê´€ë ¨ ë…¼ë¬¸

- íŒŒìƒ ëª¨ë¸ 1
    - [Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/pdf/1901.11504.pdf)
    - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
    - [ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)
    - [ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS](https://arxiv.org/pdf/2003.10555.pdf)
    - [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/pdf/1909.11942.pdf)
- íŒŒìƒ ëª¨ë¸ 2(ì§€ì‹ ì¦ë¥˜ ê¸°ë°˜)
    - [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
    - [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

---

## 11. í•œêµ­ì–´ ê´€ë ¨ LLM ëª¨ìŒ

- í•œêµ­ì–´ Taskí•  ë•Œ ì°¸ê³ í•˜ë©´ ì¢‹ì•„ìš”
    
    [í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ (Korean Pre-trained Language Models) í†ºì•„ë³´ê¸° (1)](https://www.letr.ai/blog/tech-20220908)
    
    [í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ (Korean Pre-trained Language Models) í†ºì•„ë³´ê¸° (2)](https://www.letr.ai/blog/tech-20221124)
    
    - **Encoder-Centric Models: BERT ê³„ì—´**
        
        ![[Untitled 64.png]]
        
    - **Decoder-Centric Models: GPT ê³„ì—´**
        
        ![[Untitled 65.png]]
        
    - **Encoder-Decoder Models: Seq2seq ê³„ì—´**
        
        ![[Untitled 66.png]]
        
- ê¸ˆìœµê¶Œ í•œêµ­ì–´ BERT ê¸°ë°˜ LM ëª¨ë¸
    
    - ì´ë²ˆ í•™ê¸°ì— KBêµ­ë¯¼ì€í–‰ê³¼ ì‚°í•™ í˜‘ë ¥ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.
    - ê¸ˆìœµê¶Œì—ì„œ ì–¸ì–´ëª¨ë¸ì„ ì–´ë–»ê²Œ ë§Œë“¤ê³  í™œìš©í•˜ëŠ”ì§€ ê¶ê¸ˆí•˜ì‹  ë¶„ë“¤ì€ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
    
    [KB-BERT.pdf](https://prod-files-secure.s3.us-west-2.amazonaws.com/462d6414-91e6-4eeb-9ba6-f72fd930483f/ea661402-57cf-44ea-9ecf-e3a4c0c37a1c/KB-BERT.pdf)
    

---

## 12. ë¦¬ë·° í›„ê¸°

<aside> ğŸ’¡ í‰ì†Œ NLPì— ê´€ì‹¬ì´ ë§ì•„ BERTë¥¼ ìì£¼ ë§ˆì£¼ì¹˜ê³¤ í–ˆëŠ”ë°, ì´ë²ˆ ê¸°íšŒì— ìì„¸í•˜ê²Œ ë¦¬ë·°ë¥¼ í•˜ë©´ì„œ ë‹¤ì‹œê¸ˆ ê¹Šê²Œ ì´í•´í•  ìˆ˜ ìˆì–´ ì¢‹ì€ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  í‰ì†Œ ì „ì´ í•™ìŠµ ëœ ëª¨ë¸ì„ ì£¼ë¡œ ê°€ì ¸ì™€ì„œ ì“°ë‹¤ ë³´ë‹ˆ, ë†“ì¹˜ëŠ” ì ì´ ë§ì•˜ë˜ ê²ƒ ê°™ì•˜ëŠ”ë° ì´ë²ˆ ê¸°íšŒë¡œ BERT ê³„ì—´ ëª¨ë¸ë“¤ì„ ê¾¸ì¤€íˆ ì •ë¦¬í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

</aside>